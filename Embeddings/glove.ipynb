{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gensim\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WE RAN THIS IN GOOGLE COLLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../Data/train_data.csv\")\n",
    "test_df = pd.read_csv(\"../Data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper function to calculate the embedding vector of each text\n",
    "def get_embeddings(vectors, text, k, generate_missing=False):\n",
    "    # dealing with empty text\n",
    "    if len(text)<1:\n",
    "        return np.zeros(k)\n",
    "    # generate randomized vectors for unseen words if generate_missing is True\n",
    "    if generate_missing:\n",
    "        vectorized = [vectors[word][:k] if word in vectors else np.random.rand(k) for word in text]\n",
    "    # represent unseen words with 0 vector if generate_missing is False\n",
    "    else:\n",
    "        vectorized = [vectors[word][:k] if word in vectors else np.zeros(k) for word in text]\n",
    "    # each text is represented by averaging the vectors of its constituent words\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corr(dataframe):\n",
    "  # dataframe = pd.DataFrame(dataframe.tolist())\n",
    "  # print(train_df)\n",
    "  train_df['none'] = train_df['fraudulent'].apply(lambda x: 1 if x == 0 else 0)\n",
    "  dataframe[['fraudulent', 'none']] = train_df[['fraudulent', 'none']]\n",
    "  columns = ['fraudulent', 'none']\n",
    "  features_glove = list(range(200))\n",
    "  rows_glove = [{c:dataframe[str(f)].corr(dataframe[str(c)]) for c in columns} for f in features_glove]\n",
    "  train_correlations_glove = pd.DataFrame(rows_glove, index=features_glove)\n",
    "  # print(train_correlations_glove)\n",
    "  useful = train_correlations_glove[abs(train_correlations_glove['fraudulent']) > 0.2]\n",
    "  return useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a word2vec file used for model building - 200d\n",
    "glove_input_file = \"../glove.42B.300d.txt\"\n",
    "word2vec_output_file = \"../glove.42B.300d.txt.word2vec\"\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a GloVe model\n",
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format(\"../glove.42B.300d.txt.word2vec\", binary=False, limit=1190000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embedding vectors of size 200 using tokenized text\n",
    "# side note: map function outperforms for loop\n",
    "cols_to_run = [\"tokenized_company_profile\", \"tokenized_description\", \"tokenized_requirements\", \"tokenized_benefits\"]\n",
    "\n",
    "req_train_embeddings_glove = train_df[\"tokenized_requirements\"].map(lambda x: get_embeddings(glove_model, x, 200))\n",
    "prof_train_embeddings_glove = train_df[\"tokenized_company_profile\"].map(lambda x: get_embeddings(glove_model, x, 200))\n",
    "desc_train_embeddings_glove = train_df[\"tokenized_description\"].map(lambda x: get_embeddings(glove_model, x, 200))\n",
    "ben_train_embeddings_glove = train_df[\"tokenized_benefits\"].map(lambda x: get_embeddings(glove_model, x, 200))\n",
    "\n",
    "req_train_embeddings_glove = pd.DataFrame(req_train_embeddings_glove.tolist())\n",
    "prof_train_embeddings_glove = pd.DataFrame(prof_train_embeddings_glove.tolist())\n",
    "desc_train_embeddings_glove = pd.DataFrame(desc_train_embeddings_glove.tolist())\n",
    "ben_train_embeddings_glove = pd.DataFrame(ben_train_embeddings_glove.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_test_embeddings_glove = test_df[\"tokenized_requirements\"].map(lambda x: get_embeddings(glove_model, x, 200))\n",
    "prof_test_embeddings_glove = test_df[\"tokenized_company_profile\"].map(lambda x: get_embeddings(glove_model, x, 200))\n",
    "desc_test_embeddings_glove = test_df[\"tokenized_description\"].map(lambda x: get_embeddings(glove_model, x, 200))\n",
    "ben_test_embeddings_glove = test_df[\"tokenized_benefits\"].map(lambda x: get_embeddings(glove_model, x, 200))\n",
    "\n",
    "req_test_embeddings_glove = pd.DataFrame(req_test_embeddings_glove.tolist())\n",
    "prof_test_embeddings_glove = pd.DataFrame(prof_test_embeddings_glove.tolist())\n",
    "desc_test_embeddings_glove = pd.DataFrame(desc_test_embeddings_glove.tolist())\n",
    "ben_test_embeddings_glove = pd.DataFrame(ben_test_embeddings_glove.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_glove = pd.DataFrame(embeddings_glove.tolist())\n",
    "embeddings_glove[['fraudulent', 'none']] = train_df[['fraudulent', 'none']]\n",
    "columns = ['fraudulent', 'none']\n",
    "features_glove = list(range(200))\n",
    "rows_glove = [{c:embeddings_glove[f].corr(embeddings_glove[c]) for c in columns} for f in features_glove]\n",
    "train_correlations_glove = pd.DataFrame(rows_glove, index=features_glove)\n",
    "train_correlations_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_train_embeddings_glove.to_csv('req_train_embeddings_glove.csv', index=False)\n",
    "prof_train_embeddings_glove.to_csv('prof_train_embeddings_glove.csv', index=False)\n",
    "desc_train_embeddings_glove.to_csv('desc_train_embeddings_glove.csv', index=False)\n",
    "ben_train_embeddings_glove.to_csv('ben_train_embeddings_glove.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_test_embeddings_glove.to_csv('req_test_embeddings_glove.csv', index=False)\n",
    "prof_test_embeddings_glove.to_csv('prof_test_embeddings_glove.csv', index=False)\n",
    "desc_test_embeddings_glove.to_csv('desc_test_embeddings_glove.csv', index=False)\n",
    "ben_test_embeddings_glove.to_csv('ben_test_embeddings_glove.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_train = check_corr(req_train_embeddings_glove)\n",
    "prof_train = check_corr(prof_train_embeddings_glove)\n",
    "desc_train = check_corr(desc_train_embeddings_glove)\n",
    "ben_train = check_corr(ben_train_embeddings_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(req_train))\n",
    "print(len(prof_train))\n",
    "print(len(desc_train))\n",
    "print(len(ben_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
